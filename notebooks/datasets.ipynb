{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export the Roboflow Dataset\n",
    "**First, export your dataset from Roboflow in the COCO JSON format. This will ensure compatibility between the two datasets. Roboflow allows you to export datasets in various formats, and COCO JSON format is typically used for object detection tasks.**\n",
    "\n",
    "### 2. Download the COCO Dataset\n",
    "**Download the COCO dataset from the official website. Make sure to get the annotations in the COCO JSON format as well. Depending on your specific use case (e.g., object detection), you might need only certain parts of the COCO dataset.**\n",
    "\n",
    "### 3. Consolidate Image Files\n",
    "**Place all the image files from both datasets into a single directory. This might involve renaming files to ensure there are no naming conflicts between the two datasets.**\n",
    "\n",
    "### 4. Merge Annotation Files\n",
    "**Merge the JSON files from both datasets. This can be the most complex step, depending on the specifics of the datasets. Hereâ€™s a rough Python pseudocode to guide you through merging the JSON files:**\n",
    "\n",
    "### 5. Validation\n",
    "**After merging the datasets, validate the new dataset to ensure there are no issues with the image paths or annotation formats. You can use visualization tools like CVAT or any custom scripts to check if the bounding boxes are correctly placed over the objects.**\n",
    "\n",
    "### 6. Use in Your Model\n",
    "**Once the merged dataset is validated, you can use it to train your model. Make sure to adjust any paths or configurations that reference the dataset locations.**\n",
    "\n",
    "**If you run into any specific issues while merging the datasets or have more detailed questions about the process, feel free to ask!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ./data/train.json.\n",
      "Data successfully saved to ./data/valid.json.\n",
      "Data successfully saved to ./data/train_merged.json.\n",
      "Data successfully saved to ./data/val_merged.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\" Load a JSON file and return the data. \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: The file {file_path} is not a valid JSON file.\")\n",
    "        return None\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\" Save a dictionary to a JSON file. \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "        print(f\"Data successfully saved to {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save data to {file_path}: {e}\")\n",
    "\n",
    "def change_category_id(roboflow_path, output_path):\n",
    "    \"\"\" Change category_id from 1 to 10 in Roboflow dataset. \"\"\"\n",
    "    roboflow_data = load_json(roboflow_path)\n",
    "\n",
    "    if not roboflow_data:\n",
    "        print(\"Failed to load Roboflow dataset.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Change category_id from 1 to 10 in Roboflow data\n",
    "        for ann in roboflow_data['annotations']:\n",
    "            if ann['category_id'] == 1:\n",
    "                ann['category_id'] = 10\n",
    "\n",
    "        # Save the modified Roboflow JSON file\n",
    "        save_json(roboflow_data, output_path)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e}. Please check that the JSON file contains the required keys.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def merge_datasets(coco_path, roboflow_path, output_path):\n",
    "    \"\"\" Merge COCO and Roboflow datasets. \"\"\"\n",
    "    coco_data = load_json(coco_path)\n",
    "    roboflow_data = load_json(roboflow_path)\n",
    "\n",
    "    if not coco_data or not roboflow_data:\n",
    "        print(\"Failed to load datasets. Aborting merge.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Avoid ID conflicts in images and annotations\n",
    "        max_image_id = max(img['id'] for img in coco_data['images'])\n",
    "        max_annotation_id = max(ann['id'] for ann in coco_data['annotations'])\n",
    "\n",
    "        # Increment IDs in Roboflow data to avoid duplication\n",
    "        for img in roboflow_data['images']:\n",
    "            img['id'] += max_image_id\n",
    "        for ann in roboflow_data['annotations']:\n",
    "            ann['id'] += max_annotation_id\n",
    "            ann['image_id'] += max_image_id\n",
    "\n",
    "        # Merge images and annotations\n",
    "        merged_images = coco_data['images'] + roboflow_data['images']\n",
    "        merged_annotations = coco_data['annotations'] + roboflow_data['annotations']\n",
    "\n",
    "        # Remove duplicates by ensuring unique IDs\n",
    "        unique_images = {img['id']: img for img in merged_images}.values()\n",
    "        unique_annotations = {ann['id']: ann for ann in merged_annotations}.values()\n",
    "\n",
    "        # Create a new merged dataset\n",
    "        merged_data = {\n",
    "            \"images\": list(unique_images),\n",
    "            \"annotations\": list(unique_annotations),\n",
    "            # Include other necessary fields like info, licenses, categories from the original dataset if needed\n",
    "        }\n",
    "\n",
    "        # Save the merged JSON file\n",
    "        save_json(merged_data, output_path)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Key error: {e}. Please check that both JSON files contain the required keys.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "change_category_id('./data/train/_annotations.coco.json', './data/train.json')\n",
    "change_category_id('./data/valid/_annotations.coco.json', './data/valid.json')        \n",
    "# merging for train\n",
    "merge_datasets('./data/annotations/instances_train2017.json', './data/train.json', './data/train_merged.json')\n",
    "#merging for validation\n",
    "\n",
    "merge_datasets('./data/annotations/instances_val2017.json', './data/valid.json', './data/val_merged.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting coco json to yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_coco_json_to_yolo(coco_json_path, output_dir):\n",
    "    # Load COCO JSON annotations\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Create a dictionary to hold image metadata\n",
    "    image_data = {image['id']: {'filename': image['file_name'], 'width': image['width'], 'height': image['height']} for image in data['images']}\n",
    "\n",
    "    # Process annotations\n",
    "    for annotation in data['annotations']:\n",
    "        img_id = annotation['image_id']\n",
    "        category_id = annotation['category_id'] - 1  # Adjust category_id to be zero-indexed if necessary\n",
    "        bbox = annotation['bbox']\n",
    "        # Calculate YOLO format coordinates (normalized)\n",
    "        x_center = (bbox[0] + bbox[2] / 2) / image_data[img_id]['width']\n",
    "        y_center = (bbox[1] + bbox[3] / 2) / image_data[img_id]['height']\n",
    "        width = bbox[2] / image_data[img_id]['width']\n",
    "        height = bbox[3] / image_data[img_id]['height']\n",
    "        \n",
    "        # Write to file\n",
    "        annotation_file = os.path.join(output_dir, os.path.splitext(image_data[img_id]['filename'])[0] + '.txt')\n",
    "        with open(annotation_file, 'a') as file:\n",
    "            file.write(f\"{category_id} {x_center} {y_center} {width} {height}\\n\")\n",
    "\n",
    "    # Create train.txt file listing all images\n",
    "    with open(os.path.join(output_dir, 'train.txt'), 'w') as file:\n",
    "        for img_id in image_data:\n",
    "            file_path = os.path.join(output_dir, image_data[img_id]['filename'])\n",
    "            file.write(file_path + '\\n')\n",
    "\n",
    "convert_coco_json_to_yolo('./data/Merged Datasets/train_merged.json', './data/Merged Datasets/train-merged/labels')\n",
    "convert_coco_json_to_yolo('./data/Merged Datasets/val_merged.json', './data/Merged Datasets/val-merged/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "data = {\n",
    "    \"train\": \"train-merged/images\",\n",
    "    \"val\": \"val-merged/images\",\n",
    "    \"nc\": 91,  # Update with the number of unique classes in your dataset\n",
    "    \"names\": [\n",
    "\"person\",\n",
    "\"bicycle\",\n",
    "\"car\",\n",
    "\"motorcycle\",\n",
    "\"airplane\",\n",
    "\"bus\",\n",
    "\"train\",\n",
    "\"truck\",\n",
    "\"boat\",\n",
    "\"traffic light\",\n",
    "\"fire hydrant\",\n",
    "\"street sign\",\n",
    "\"stop sign\",\n",
    "\"parking meter\",\n",
    "\"bench\",\n",
    "\"bird\",\n",
    "\"cat\",\n",
    "\"dog\",\n",
    "\"horse\",\n",
    "\"sheep\",\n",
    "\"cow\",\n",
    "\"elephant\",\n",
    "\"bear\",\n",
    "\"zebra\",\n",
    "\"giraffe\",\n",
    "\"hat\",\n",
    "\"backpack\",\n",
    "\"umbrella\",\n",
    "\"shoe\",\n",
    "\"eye glasses\",\n",
    "\"handbag\",\n",
    "\"tie\",\n",
    "\"suitcase\",\n",
    "\"frisbee\",\n",
    "\"skis\",\n",
    "\"snowboard\",\n",
    "\"sports ball\",\n",
    "\"kite\",\n",
    "\"baseball bat\",\n",
    "\"baseball glove\",\n",
    "\"skateboard\",\n",
    "\"surfboard\",\n",
    "\"tennis racket\",\n",
    "\"bottle\",\n",
    "\"plate\",\n",
    "\"wine glass\",\n",
    "\"cup\",\n",
    "\"fork\",\n",
    "\"knife\",\n",
    "\"spoon\",\n",
    "\"bowl\",\n",
    "\"banana\",\n",
    "\"apple\",\n",
    "\"sandwich\",\n",
    "\"orange\",\n",
    "\"broccoli\",\n",
    "\"carrot\",\n",
    "\"hot dog\",\n",
    "\"pizza\",\n",
    "\"donut\",\n",
    "\"cake\",\n",
    "\"chair\",\n",
    "\"couch\",\n",
    "\"potted plant\",\n",
    "\"bed\",\n",
    "\"mirror\",\n",
    "\"dining table\",\n",
    "\"window\",\n",
    "\"desk\",\n",
    "\"toilet\",\n",
    "\"door\",\n",
    "\"tv\",\n",
    "\"laptop\",\n",
    "\"mouse\",\n",
    "\"remote\",\n",
    "\"keyboard\",\n",
    "\"cell phone\",\n",
    "\"microwave\",\n",
    "\"oven\",\n",
    "\"toaster\",\n",
    "\"sink\",\n",
    "\"refrigerator\",\n",
    "\"blender\",\n",
    "\"book\",\n",
    "\"clock\",\n",
    "\"vase\",\n",
    "\"scissors\",\n",
    "\"teddy bear\",\n",
    "\"hair drier\",\n",
    "\"toothbrush\",\n",
    "\"hair brush\"\n",
    "]\n",
    "}\n",
    "\n",
    "# Write the dictionary to a YAML file\n",
    "with open('data.yaml', 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
